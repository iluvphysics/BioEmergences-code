{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1b300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from tifffile import imread\n",
    "from csbdeep.utils import Path, download_and_extract_zip_file, plot_some, axes_dict, plot_history\n",
    "from csbdeep.io import save_tiff_imagej_compatible, load_training_data\n",
    "from csbdeep.models import CARE, Config\n",
    "from csbdeep.data import RawData, create_patches, create_patches_reduced_target\n",
    "import shutil \n",
    "import os\n",
    "# # renomme fichiers pour csbdeep \n",
    "# import os\n",
    "\n",
    "date = \"070418a\"\n",
    "path = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/data/train/train_samples\"\n",
    "pathUniform_train = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataUniform/train/train_samples\"\n",
    "pathUniform_test = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataUniform/test/test_samples\"\n",
    "\n",
    "def renomme_data(date, tiffdata_path, demi_pas=False) :\n",
    "    \n",
    "    if demi_pas :\n",
    "        return None \n",
    "    \n",
    "    for file_name in os.listdir(tiffdata_path) :\n",
    "        \n",
    "        if file_name[:len(date)] != date or file_name[-4:] != \".tif\" :\n",
    "            continue\n",
    "        \n",
    "        # nbre de caractères du nombre \n",
    "        s=0\n",
    "        while file_name[len(date) + 2 + s] != \"_\" :\n",
    "            s+= 1\n",
    "        assert s%2 == 0\n",
    "        \n",
    "        \n",
    "        nbre =  ( int(file_name[len(date)+2:len(date)+2+s//2]) + int(file_name[len(date)+2+s//2:len(date)+2+s]) ) // 2    \n",
    "        nvnom = file_name[:len(date)+2] + str(nbre).zfill(3) + file_name[len(date)+2+s:]\n",
    "        \n",
    "        path_old = tiffdata_path + \"/\" + file_name\n",
    "        path_new = tiffdata_path + \"/\" + nvnom\n",
    "        print(nvnom, nbre)\n",
    "        \n",
    "        #os.rename(path_old, path_new)\n",
    "        \n",
    "    return None\n",
    "\n",
    "# données\n",
    "dir_data = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/data\"\n",
    "dir_dataUniform = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataUniform\" \n",
    "dir_dataDemistep = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataDemistep\" \n",
    "dir_dataMix = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataMix\"\n",
    "\n",
    "def fetch_rawdata(dir_data):\n",
    "    raw_data = RawData.from_folder (\n",
    "        basepath = dir_data + \"/train\",\n",
    "        source_dirs = [dir_data + \"/train/train_samples\"],\n",
    "        target_dir = dir_data + \"/train/train_labels\",\n",
    "        axes = \"ZCYX\" )\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "#renomme_data(pathUniform_train, demi_pas=False)\n",
    "#renomme_data(pathUniform_test, demi_pas=False)\n",
    "rawdata = fetch_rawdata(dir_dataMix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531ee3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_file_dir = dir_dataMix+\"/my_training_data.npz\"\n",
    "\n",
    "# création des patches\n",
    "X, Y, XY_axes = create_patches_reduced_target(\n",
    "    reduction_axes      = \"C\",\n",
    "    target_axes         = \"ZYX\",\n",
    "    raw_data            = rawdata,\n",
    "    patch_size          = (32,4,64,64),\n",
    "    n_patches_per_image = 120,\n",
    "    save_file           = save_file_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d7a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training images:\t 21492\n",
      "number of validation images:\t 2388\n",
      "image size (3D):\t\t (32, 64, 64)\n",
      "axes:\t\t\t\t SZYXC\n",
      "channels in / out:\t\t 4 / 1\n"
     ]
    }
   ],
   "source": [
    "#dir_dataDemistep = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataDemistep\" \n",
    "dir_dataMix = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/dataMix\" \n",
    "data_path = dir_dataMix+\"/my_training_data.npz\"\n",
    "(X,Y), (X_val,Y_val), axes = load_training_data(data_path, validation_split=0.1, \n",
    "                                                verbose=True)\n",
    "c = axes_dict(axes)['C']\n",
    "n_channel_in, n_channel_out = X.shape[c], Y.shape[c]\n",
    "config = Config(axes, n_channel_in, n_channel_out, train_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc647cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioAMD\\anaconda3\\envs\\care\\lib\\site-packages\\csbdeep\\models\\base_model.py:149: UserWarning: output path for model already exists, files may be overwritten: C:\\Users\\bioAMD\\Desktop\\Nathan\\modele_interpolation5\n",
      "  warnings.warn('output path for model already exists, files may be overwritten: %s' % str(self.logdir.resolve()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fc2bb0b96870965c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fc2bb0b96870965c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4611F0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4611F0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names.\n",
      "Match 0:\n",
      "lambda x: K.mean(x, axis=-1)\n",
      "\n",
      "Match 1:\n",
      "lambda x: x\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4611F0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4611F0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names.\n",
      "Match 0:\n",
      "lambda x: K.mean(x, axis=-1)\n",
      "\n",
      "Match 1:\n",
      "lambda x: x\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4613A0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4613A0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names.\n",
      "Match 0:\n",
      "lambda x: K.mean(x, axis=-1)\n",
      "\n",
      "Match 1:\n",
      "lambda x: x\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4613A0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4613A0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names.\n",
      "Match 0:\n",
      "lambda x: K.mean(x, axis=-1)\n",
      "\n",
      "Match 1:\n",
      "lambda x: x\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4614C0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4614C0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names.\n",
      "Match 0:\n",
      "lambda x: K.mean(x, axis=-1)\n",
      "\n",
      "Match 1:\n",
      "lambda x: x\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4614C0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _mean_or_not.<locals>.<lambda> at 0x0000020ADA4614C0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names.\n",
      "Match 0:\n",
      "lambda x: K.mean(x, axis=-1)\n",
      "\n",
      "Match 1:\n",
      "lambda x: x\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  6/400 [..............................] - ETA: 2:54 - loss: 0.0948 - mse: 0.0318 - mae: 0.0948WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1432s vs `on_train_batch_end` time: 0.2513s). Check your callbacks.\n",
      "400/400 [==============================] - 224s 540ms/step - loss: 0.0538 - mse: 0.0090 - mae: 0.0538 - val_loss: 0.0516 - val_mse: 0.0079 - val_mae: 0.0516\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 207s 517ms/step - loss: 0.0509 - mse: 0.0079 - mae: 0.0509 - val_loss: 0.0508 - val_mse: 0.0078 - val_mae: 0.0508\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 207s 518ms/step - loss: 0.0505 - mse: 0.0077 - mae: 0.0505 - val_loss: 0.0504 - val_mse: 0.0076 - val_mae: 0.0504\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 207s 518ms/step - loss: 0.0508 - mse: 0.0078 - mae: 0.0508 - val_loss: 0.0503 - val_mse: 0.0076 - val_mae: 0.0503\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 209s 523ms/step - loss: 0.0501 - mse: 0.0076 - mae: 0.0501 - val_loss: 0.0502 - val_mse: 0.0075 - val_mae: 0.0502\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 208s 520ms/step - loss: 0.0501 - mse: 0.0076 - mae: 0.0501 - val_loss: 0.0500 - val_mse: 0.0075 - val_mae: 0.0500\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 209s 523ms/step - loss: 0.0499 - mse: 0.0075 - mae: 0.0499 - val_loss: 0.0500 - val_mse: 0.0074 - val_mae: 0.0500\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 211s 528ms/step - loss: 0.0500 - mse: 0.0075 - mae: 0.0500 - val_loss: 0.0500 - val_mse: 0.0074 - val_mae: 0.0500\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 209s 523ms/step - loss: 0.0502 - mse: 0.0076 - mae: 0.0502 - val_loss: 0.0499 - val_mse: 0.0075 - val_mae: 0.0499\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 215s 537ms/step - loss: 0.0500 - mse: 0.0075 - mae: 0.0500 - val_loss: 0.0499 - val_mse: 0.0074 - val_mae: 0.0499\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 209s 522ms/step - loss: 0.0500 - mse: 0.0075 - mae: 0.0500 - val_loss: 0.0499 - val_mse: 0.0073 - val_mae: 0.0499\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 251s 629ms/step - loss: 0.0500 - mse: 0.0075 - mae: 0.0500 - val_loss: 0.0497 - val_mse: 0.0074 - val_mae: 0.0497\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 0.0494 - mse: 0.0073 - mae: 0.0494 - val_loss: 0.0498 - val_mse: 0.0074 - val_mae: 0.0498\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 224s 560ms/step - loss: 0.0496 - mse: 0.0074 - mae: 0.0496 - val_loss: 0.0498 - val_mse: 0.0073 - val_mae: 0.0498\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 223s 558ms/step - loss: 0.0495 - mse: 0.0073 - mae: 0.0495 - val_loss: 0.0497 - val_mse: 0.0073 - val_mae: 0.0497\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 239s 597ms/step - loss: 0.0499 - mse: 0.0074 - mae: 0.0499 - val_loss: 0.0497 - val_mse: 0.0073 - val_mae: 0.0497\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 235s 587ms/step - loss: 0.0497 - mse: 0.0073 - mae: 0.0497 - val_loss: 0.0497 - val_mse: 0.0073 - val_mae: 0.0497\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 224s 561ms/step - loss: 0.0496 - mse: 0.0073 - mae: 0.0496 - val_loss: 0.0497 - val_mse: 0.0072 - val_mae: 0.0497\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 221s 553ms/step - loss: 0.0496 - mse: 0.0073 - mae: 0.0496 - val_loss: 0.0497 - val_mse: 0.0073 - val_mae: 0.0497\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 209s 523ms/step - loss: 0.0496 - mse: 0.0073 - mae: 0.0496 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 250s 625ms/step - loss: 0.0498 - mse: 0.0073 - mae: 0.0498 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 222s 554ms/step - loss: 0.0494 - mse: 0.0073 - mae: 0.0494 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 226s 564ms/step - loss: 0.0494 - mse: 0.0073 - mae: 0.0494 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 221s 552ms/step - loss: 0.0495 - mse: 0.0073 - mae: 0.0495 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 208s 520ms/step - loss: 0.0494 - mse: 0.0072 - mae: 0.0494 - val_loss: 0.0497 - val_mse: 0.0072 - val_mae: 0.0497\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 253s 634ms/step - loss: 0.0497 - mse: 0.0073 - mae: 0.0497 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 252s 631ms/step - loss: 0.0493 - mse: 0.0072 - mae: 0.0493 - val_loss: 0.0495 - val_mse: 0.0072 - val_mae: 0.0495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "400/400 [==============================] - 210s 526ms/step - loss: 0.0496 - mse: 0.0073 - mae: 0.0496 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 215s 538ms/step - loss: 0.0491 - mse: 0.0072 - mae: 0.0491 - val_loss: 0.0495 - val_mse: 0.0072 - val_mae: 0.0495\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 214s 534ms/step - loss: 0.0498 - mse: 0.0073 - mae: 0.0498 - val_loss: 0.0496 - val_mse: 0.0073 - val_mae: 0.0496\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 247s 618ms/step - loss: 0.0492 - mse: 0.0072 - mae: 0.0492 - val_loss: 0.0496 - val_mse: 0.0072 - val_mae: 0.0496\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 228s 570ms/step - loss: 0.0493 - mse: 0.0072 - mae: 0.0493 - val_loss: 0.0495 - val_mse: 0.0073 - val_mae: 0.0495\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 239s 597ms/step - loss: 0.0495 - mse: 0.0072 - mae: 0.0495 - val_loss: 0.0495 - val_mse: 0.0073 - val_mae: 0.0495\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 244s 611ms/step - loss: 0.0492 - mse: 0.0072 - mae: 0.0492 - val_loss: 0.0495 - val_mse: 0.0072 - val_mae: 0.0495\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 225s 563ms/step - loss: 0.0491 - mse: 0.0072 - mae: 0.0491 - val_loss: 0.0495 - val_mse: 0.0072 - val_mae: 0.0495\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 357s 893ms/step - loss: 0.0492 - mse: 0.0071 - mae: 0.0492 - val_loss: 0.0494 - val_mse: 0.0072 - val_mae: 0.0494\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 375s 939ms/step - loss: 0.0498 - mse: 0.0073 - mae: 0.0498 - val_loss: 0.0495 - val_mse: 0.0073 - val_mae: 0.0495\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 388s 963ms/step - loss: 0.0494 - mse: 0.0072 - mae: 0.0494 - val_loss: 0.0495 - val_mse: 0.0072 - val_mae: 0.0495\n",
      "Epoch 39/100\n",
      "318/400 [======================>.......] - ETA: 1:18 - loss: 0.0491 - mse: 0.0071 - mae: 0.0491"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--logdir \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/bioAMD/Desktop/Nathan\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m }\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m savehist_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/bioAMD/Desktop/Nathan/Interpolation/trainHistoryDict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ind) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m epoch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\csbdeep\\models\\care_standard.py:196\u001b[0m, in \u001b[0;36mCARE.train\u001b[1;34m(self, X, Y, validation_data, epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m    193\u001b[0m training_data \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mDataWrapper(X, Y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_batch_size, length\u001b[38;5;241m=\u001b[39mepochs\u001b[38;5;241m*\u001b[39msteps_per_epoch)\n\u001b[0;32m    195\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model\u001b[38;5;241m.\u001b[39mfit_generator \u001b[38;5;28;01mif\u001b[39;00m IS_TF_1 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model\u001b[38;5;241m.\u001b[39mfit\n\u001b[1;32m--> 196\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_finished()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1188\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1188\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1190\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:457\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 457\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:337\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    334\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    335\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    340\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:375\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    374\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 375\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    378\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1029\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1029\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1101\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1097\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1100\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:519\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    517\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:867\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    863\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    864\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 867\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    868\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:867\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    863\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    864\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 867\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    868\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:515\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 515\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    517\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\care\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ind = 5\n",
    "model = CARE(config, 'modele_interpolation'+str(ind))\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir { \"C:/Users/bioAMD/Desktop/Nathan\" }\n",
    "history = model.train(X,Y, validation_data = (X_val, Y_val))\n",
    "\n",
    "savehist_path = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/trainHistoryDict\" + str(ind) + \".txt\"\n",
    "epoch = np.arange(start=1, stop=len(history.history[\"loss\"])+1, step=1 )\n",
    "\n",
    "if os.path.exists(savehist_path) :\n",
    "    os.remove(savehist_path)\n",
    "with open(savehist_path, 'w') as file_pi:\n",
    "    file_pi.write(\"# epoch loss mse mae val_loss val_mse val_mae \\n\")\n",
    "    for i in range(len(history.history[\"loss\"])) :\n",
    "        file_pi.write(str(epoch[i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"loss\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"mse\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"mae\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"val_loss\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"val_mse\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"val_mae\"][i]) + \"\\n\")\n",
    "\n",
    "#ind += 1 # pour séparer les history de deux entrainements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925457a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#savehist_path = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/trainHistoryDict\" + str(ind) + \".txt\"\n",
    "model = CARE(config, 'modele_interpolation'+str(ind))\n",
    "\n",
    "savehist_path = \"C:/Users/bioAMD/Desktop/Nathan/Interpolation/trainHistoryDict\" + str(ind) + \".txt\"\n",
    "epoch = np.arange(start=1, stop=32, step=1 )\n",
    "\n",
    "if os.path.exists(savehist_path) :\n",
    "    os.remove(savehist_path)\n",
    "with open(savehist_path, 'w') as file_pi:\n",
    "    file_pi.write(\"# epoch loss mse mae val_loss val_mse val_mae \\n\")\n",
    "    for i in range(len(history.history[\"loss\"])) :\n",
    "        file_pi.write(str(epoch[i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"loss\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"mse\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"mae\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"val_loss\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"val_mse\"][i]) + \" \")\n",
    "        file_pi.write(str(history.history[\"val_mae\"][i]) + \"\\n\")\n",
    "\n",
    "xy_patchsize = 64\n",
    "batchsize_max = 7.7*1e3 // ( 32*(xy_patchsize**2) *4* 1e-6)\n",
    "print(batchsize_max)\n",
    "\n",
    "#with open('/trainHistoryDict' + str(ind), \"rb\") as file_pi:\n",
    "#    history = pickle.load(file_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
